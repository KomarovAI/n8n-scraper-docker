name: Playwright Batch Scraper (Enhanced)

on:
  workflow_dispatch:
    inputs:
      urls:
        description: 'JSON array of URL objects'
        required: true
      batchId:
        description: 'Batch ID for tracking'
        required: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium --with-deps
      
      - name: Run Enhanced Playwright Scraper
        env:
          URLS_JSON: ${{ github.event.inputs.urls }}
          BATCH_ID: ${{ github.event.inputs.batchId }}
        run: |
          node scripts/playwright_batch_scraper.js
      
      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-batch-${{ github.event.inputs.batchId }}
          path: results.json
          retention-days: 1
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-logs-batch-${{ github.event.inputs.batchId }}
          path: scraper.log
          retention-days: 1
