name: Parallel Scraping (Dynamic Runners)

on:
  workflow_dispatch:
    inputs:
      target_urls:
        description: 'URLs –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)'
        required: true
        default: 'https://example.com,https://test.com,https://demo.com'
      max_parallel_runners:
        description: '–ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞–Ω–Ω–µ—Ä–æ–≤'
        required: true
        default: '2'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
          - '5'
          - '10'
      use_proxy:
        description: '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏'
        required: true
        default: 'true'
        type: boolean

permissions:
  contents: read
  actions: write

jobs:
  # Job 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á
  prepare-scraping-tasks:
    name: üìã –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–¥–∞—á
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      task_count: ${{ steps.set-matrix.outputs.count }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Generate dynamic matrix
        id: set-matrix
        run: |
          # –ü–∞—Ä—Å–∏–º URLs –∏–∑ input
          IFS=',' read -ra URLS <<< "${{ github.event.inputs.target_urls }}"
          
          # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON matrix
          MATRIX="{\"include\":["
          INDEX=0
          for url in "${URLS[@]}"; do
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ (–µ—Å–ª–∏ enabled)
            if [ "${{ github.event.inputs.use_proxy }}" = "true" ]; then
              PROXY="http://proxy-$((INDEX % 3 + 1)).example.com:8080"
            else
              PROXY="none"
            fi
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ matrix
            MATRIX="$MATRIX{\"url\":\"${url}\",\"proxy\":\"${PROXY}\",\"index\":$INDEX}"
            INDEX=$((INDEX + 1))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—è—Ç—É—é –µ—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç
            if [ $INDEX -lt ${#URLS[@]} ]; then
              MATRIX="$MATRIX,"
            fi
          done
          MATRIX="$MATRIX]}"
          
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "count=${#URLS[@]}" >> $GITHUB_OUTPUT
          echo "Generated matrix: $MATRIX"
          echo "Total tasks: ${#URLS[@]}"

  # Job 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π scraping —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π
  scrape:
    name: üåê Scrape [${{ matrix.index }}] ${{ matrix.url }}
    needs: prepare-scraping-tasks
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–∏–Ω —Ä–∞–Ω–Ω–µ—Ä —É–ø–∞–ª
      max-parallel: ${{ fromJSON(github.event.inputs.max_parallel_runners) }}
      matrix: ${{ fromJSON(needs.prepare-scraping-tasks.outputs.matrix) }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Install Puppeteer and dependencies
        run: |
          npm install puppeteer puppeteer-extra puppeteer-extra-plugin-stealth
      
      - name: Run scraper with proxy
        timeout-minutes: 10
        env:
          TARGET_URL: ${{ matrix.url }}
          PROXY_URL: ${{ matrix.proxy }}
          TASK_INDEX: ${{ matrix.index }}
        run: |
          echo "üöÄ Starting scraper for: $TARGET_URL"
          echo "üîí Using proxy: $PROXY_URL"
          echo "üìä Task index: $TASK_INDEX"
          
          # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç
          cat > scrape-task.js << 'EOF'
          const puppeteer = require('puppeteer-extra');
          const StealthPlugin = require('puppeteer-extra-plugin-stealth');
          puppeteer.use(StealthPlugin());
          
          (async () => {
            const url = process.env.TARGET_URL;
            const proxy = process.env.PROXY_URL;
            const index = process.env.TASK_INDEX;
            
            console.log(`[${index}] Scraping: ${url}`);
            
            const launchOptions = {
              headless: 'new',
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            };
            
            // –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if (proxy !== 'none') {
              launchOptions.args.push(`--proxy-server=${proxy}`);
              console.log(`[${index}] Using proxy: ${proxy}`);
            }
            
            const browser = await puppeteer.launch(launchOptions);
            const page = await browser.newPage();
            
            try {
              await page.goto(url, { waitUntil: 'networkidle2', timeout: 30000 });
              const title = await page.title();
              const content = await page.content();
              
              console.log(`[${index}] ‚úÖ Success: ${title}`);
              console.log(`[${index}] Content length: ${content.length} bytes`);
              
              // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
              const fs = require('fs');
              fs.writeFileSync(`result-${index}.html`, content);
              
            } catch (error) {
              console.error(`[${index}] ‚ùå Error: ${error.message}`);
              process.exit(1);
            } finally {
              await browser.close();
            }
          })();
          EOF
          
          # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∏–ø—Ç
          node scrape-task.js
      
      - name: Upload scraping results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-result-${{ matrix.index }}
          path: result-*.html
          retention-days: 7

  # Job 3: –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
  summary:
    name: üìä –ò—Ç–æ–≥–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
    needs: [prepare-scraping-tasks, scrape]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./results
      
      - name: Generate summary
        run: |
          echo "## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**–í—Å–µ–≥–æ –∑–∞–¥–∞—á:** ${{ needs.prepare-scraping-tasks.outputs.task_count }}" >> $GITHUB_STEP_SUMMARY
          echo "**–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞–Ω–Ω–µ—Ä–æ–≤:** ${{ github.event.inputs.max_parallel_runners }}" >> $GITHUB_STEP_SUMMARY
          echo "**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏:** ${{ github.event.inputs.use_proxy }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìÅ –°–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:" >> $GITHUB_STEP_SUMMARY
          find ./results -name "*.html" | while read file; do
            size=$(wc -c < "$file")
            echo "- $(basename $file): ${size} bytes" >> $GITHUB_STEP_SUMMARY
          done
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!" >> $GITHUB_STEP_SUMMARY
