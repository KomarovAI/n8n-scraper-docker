{"name": "Smart Web Scraper - Production v2 (Enhanced)","nodes": [{"parameters": {"httpMethod": "POST","path": "scrape","authentication": "headerAuth","options": {}},"name": "Webhook (Header Auth)","type": "n8n-nodes-base.webhook","position": [250, 300],"webhookId": "scraper-webhook","credentials": {"httpHeaderAuth": {"id": "1","name": "Scraper API Key"}}},{"parameters": {"functionCode": "// ENHANCED INPUT VALIDATION + SSRF PROTECTION\\nconst urls = Array.isArray($json.urls) ? $json.urls : [$json.url];\\nconst selector = $json.selector || 'main, article, .content, body';\\nconst waitFor = $json.waitFor;\\nconst extractImages = $json.extractImages || false;\\n\\n// Validate URLs array\\nif (!urls || urls.length === 0) {\\n  throw new Error('No URLs provided');\\n}\\n\\n// SSRF Protection\\nconst blockedHosts = [\\n  'localhost', '127.0.0.1', '0.0.0.0', '10.0.0.0/8',\\n  '172.16.0.0/12', '192.168.0.0/16',\\n  '169.254.169.254',  // AWS metadata\\n  'metadata.google.internal',  // GCP metadata\\n  'metadata.azure.com'  // Azure metadata\\n];\\n\\nconst validUrls = [];\\nconst invalidUrls = [];\\n\\nfor (const url of urls) {\\n  try {\\n    if (!url || !url.startsWith('http')) {\\n      invalidUrls.push({ url, reason: 'Invalid URL format' });\\n      continue;\\n    }\\n    \\n    const urlObj = new URL(url);\\n    \\n    // Check blocked hosts\\n    if (blockedHosts.some(blocked => urlObj.hostname.includes(blocked))) {\\n      invalidUrls.push({ url, reason: 'SSRF detected - blocked host' });\\n      continue;\\n    }\\n    \\n    // Detect site characteristics\\n    const needsJS = ['javascript', 'react', 'vue', 'angular', 'spa', 'ajax'].some(\\n      keyword => url.toLowerCase().includes(keyword)\\n    );\\n    \\n    const isProtected = ['cloudflare', 'datadome', 'akamai', 'captcha'].some(\\n      keyword => url.toLowerCase().includes(keyword)\\n    );\\n    \\n    validUrls.push({\\n      url,\\n      selector,\\n      waitFor,\\n      extractImages,\\n      needsJS,\\n      isProtected,\\n      requestId: `scrape-${Date.now()}-${validUrls.length}`,\\n      timestamp: new Date().toISOString()\\n    });\\n    \\n  } catch (error) {\\n    invalidUrls.push({ url, reason: error.message });\\n  }\\n}\\n\\nif (validUrls.length === 0) {\\n  throw new Error('No valid URLs to scrape');\\n}\\n\\nreturn {\\n  json: {\\n    validUrls,\\n    invalidUrls,\\n    total: urls.length,\\n    valid: validUrls.length,\\n    invalid: invalidUrls.length\\n  }\\n};"},"name": "Input Validator (Enhanced)","type": "n8n-nodes-base.code","position": [450, 300]},{"parameters": {"functionCode": "// SMART BATCH SPLITTING\\nconst validUrls = $json.validUrls;\\nconst BATCH_SIZE = 100;\\nconst batches = [];\\n\\nfor (let i = 0; i < validUrls.length; i += BATCH_SIZE) {\\n  batches.push({\\n    batchId: Math.floor(i / BATCH_SIZE) + 1,\\n    urls: validUrls.slice(i, i + BATCH_SIZE),\\n    batchSize: Math.min(BATCH_SIZE, validUrls.length - i)\\n  });\\n}\\n\\nreturn batches.map(batch => ({ json: batch }));"},"name": "Split Into Batches","type": "n8n-nodes-base.code","position": [650, 300]},{"parameters": {"conditions": {"boolean": [{"value1": "={{$json.urls[0].isProtected}}","value2": true}]}},"name": "Protected Site?","type": "n8n-nodes-base.if","position": [850, 300]},{"parameters": {"url": "https://api.github.com/repos/{{$env.GITHUB_OWNER}}/{{$env.GITHUB_REPO}}/actions/workflows/nodriver-batch.yml/dispatches","authentication": "predefinedCredentialType","nodeCredentialType": "githubApi","sendBody": true,"bodyParameters": {"parameters": [{"name": "ref","value": "main"},{"name": "inputs","value": "={{JSON.stringify({urls: $json.urls, batchId: $json.batchId})}}"]},"options": {"timeout": 120000}},"name": "Trigger Nodriver Batch (GitHub Actions)","type": "n8n-nodes-base.httpRequest","position": [1050, 200],"credentials": {"githubApi": {"id": "2","name": "GitHub API"}}},{"parameters": {"conditions": {"boolean": [{"value1": "={{$json.urls[0].needsJS}}","value2": true}]}},"name": "Needs JavaScript?","type": "n8n-nodes-base.if","position": [1050, 350]},{"parameters": {"url": "https://api.github.com/repos/{{$env.GITHUB_OWNER}}/{{$env.GITHUB_REPO}}/actions/workflows/playwright-batch.yml/dispatches","authentication": "predefinedCredentialType","nodeCredentialType": "githubApi","sendBody": true,"bodyParameters": {"parameters": [{"name": "ref","value": "main"},{"name": "inputs","value": "={{JSON.stringify({urls: $json.urls, batchId: $json.batchId})}}"]}},"name": "Trigger Playwright Batch (GitHub Actions)","type": "n8n-nodes-base.httpRequest","position": [1250, 300]},{"parameters": {"functionCode": "// BATCH HTTP REQUESTS (for simple sites)\\nconst axios = require('axios');\\nconst urls = $json.urls;\\nconst results = [];\\n\\nfor (const urlData of urls) {\\n  try {\\n    const response = await axios.get(urlData.url, {\\n      timeout: 30000,\\n      headers: {\\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',\\n        'Accept': 'text/html,application/xhtml+xml'\\n      }\\n    });\\n    \\n    results.push({\\n      url: urlData.url,\\n      success: true,\\n      html: response.data.substring(0, 100000),\\n      status: response.status,\\n      runner: 'http_basic',\\n      requestId: urlData.requestId\\n    });\\n  } catch (error) {\\n    results.push({\\n      url: urlData.url,\\n      success: false,\\n      error: error.message,\\n      runner: 'http_basic',\\n      requestId: urlData.requestId\\n    });\\n  }\\n}\\n\\nreturn { json: { results, batchId: $json.batchId } };"},"name": "Basic HTTP Batch","type": "n8n-nodes-base.code","position": [1250, 450]},{"parameters": {"functionCode": "// ENHANCED POLLING with EXPONENTIAL BACKOFF\\nconst batchId = $json.batchId;\\nconst maxAttempts = 30; // 30 * 20s = 10 min\\nconst baseDelay = 20000; // 20 seconds\\n\\nfor (let i = 0; i < maxAttempts; i++) {\\n  const delay = Math.min(baseDelay * Math.pow(1.2, i), 60000); // Max 60s\\n  await new Promise(resolve => setTimeout(resolve, delay));\\n  \\n  try {\\n    const response = await this.helpers.httpRequest({\\n      method: 'GET',\\n      url: `https://api.github.com/repos/${process.env.GITHUB_OWNER}/${process.env.GITHUB_REPO}/actions/artifacts`,\\n      headers: {\\n        'Authorization': `Bearer ${process.env.GITHUB_TOKEN}`,\\n        'Accept': 'application/vnd.github+json'\\n      }\\n    });\\n    \\n    const artifact = response.artifacts.find(a => a.name.includes(`batch-${batchId}`));\\n    \\n    if (artifact) {\\n      return {\\n        json: {\\n          status: 'completed',\\n          artifactUrl: artifact.archive_download_url,\\n          artifactId: artifact.id,\\n          batchId,\\n          attempts: i + 1,\\n          totalWaitTime: delay * (i + 1)\\n        }\\n      };\\n    }\\n  } catch (error) {\\n    console.log(`Polling attempt ${i + 1} failed: ${error.message}`);\\n  }\\n}\\n\\nthrow new Error(`Timeout: batch ${batchId} did not complete in 10 minutes`);"},"name": "Poll GitHub Actions Status","type": "n8n-nodes-base.code","position": [1450, 250]},{"parameters": {"url": "={{$json.artifactUrl}}","authentication": "predefinedCredentialType","nodeCredentialType": "githubApi","options": {"response": {"response": {"responseFormat": "file"}}}},"name": "Download Artifact","type": "n8n-nodes-base.httpRequest","position": [1650, 250]},{"parameters": {"functionCode": "// PARSE ARTIFACT & EXTRACT RESULTS\\nconst binaryData = await this.helpers.getBinaryDataBuffer(0);\\nconst results = JSON.parse(binaryData.toString());\\n\\nreturn {\\n  json: {\\n    batchId: $json.batchId,\\n    results: results.successful || [],\\n    failed: results.failed || [],\\n    stats: {\\n      total: (results.successful?.length || 0) + (results.failed?.length || 0),\\n      successful: results.successful?.length || 0,\\n      failed: results.failed?.length || 0\\n    }\\n  }\\n};"},"name": "Parse Results","type": "n8n-nodes-base.code","position": [1850, 300]},{"parameters": {"functionCode": "// SMART CONTENT EXTRACTION with CLEANUP\\nconst cheerio = require('cheerio');\\nconst items = $input.all();\\nconst extracted = [];\\n\\nfor (const item of items) {\\n  const results = item.json.results || [item.json];\\n  \\n  for (const result of results) {\\n    if (!result.success || !result.html) continue;\\n    \\n    try {\\n      const $ = cheerio.load(result.html);\\n      \\n      // Priority-based content extraction\\n      const mainContent = \\n        $('main').text().trim() ||\\n        $('article').text().trim() ||\\n        $('.content, #content').text().trim() ||\\n        $('body').text().trim();\\n      \\n      // Clean text\\n      const cleanText = mainContent\\n        .replace(/\\\\s+/g, ' ')\\n        .replace(/\\\\n+/g, '\\\\n')\\n        .trim()\\n        .substring(0, 50000);\\n      \\n      // Extract metadata\\n      const title = $('title').text() || $('meta[property=\\"og:title\\"]').attr('content') || '';\\n      const description = $('meta[name=\\"description\\"]').attr('content') || '';\\n      \\n      // Extract links (limited)\\n      const links = [];\\n      $('a[href]').slice(0, 100).each((i, el) => {\\n        const href = $(el).attr('href');\\n        if (href && !href.startsWith('#')) {\\n          links.push({ url: href, text: $(el).text().trim() });\\n        }\\n      });\\n      \\n      extracted.push({\\n        url: result.url,\\n        success: true,\\n        runner: result.runner,\\n        data: {\\n          title,\\n          description,\\n          text_content: cleanText,\\n          links,\\n          meta: {\\n            text_length: cleanText.length,\\n            links_count: links.length\\n          }\\n        },\\n        timestamp: new Date().toISOString()\\n      });\\n      \\n    } catch (error) {\\n      extracted.push({\\n        url: result.url,\\n        success: false,\\n        error: `Extraction failed: ${error.message}`,\\n        runner: result.runner\\n      });\\n    }\\n  }\\n}\\n\\nreturn extracted.map(item => ({ json: item }));"},"name": "Extract & Clean Content","type": "n8n-nodes-base.code","position": [2050, 300]},{"parameters": {"conditions": {"number": [{"value1": "={{$json.data?.text_length || 0}}","operation": "larger","value2": 100}]}},"name": "Quality Check","type": "n8n-nodes-base.if","position": [2250, 300]},{"parameters": {"functionCode": "// FIRECRAWL FALLBACK with RETRY\\nconst failedItems = $input.all().filter(item => !item.json.success || (item.json.data?.text_length || 0) < 100);\\n\\nif (failedItems.length === 0) {\\n  return [];\\n}\\n\\nconst axios = require('axios');\\nconst results = [];\\nconst MAX_RETRIES = 3;\\n\\nasync function retryFirecrawl(url, retries = MAX_RETRIES) {\\n  for (let attempt = 0; attempt < retries; attempt++) {\\n    try {\\n      const response = await axios.post(\\n        'https://api.firecrawl.dev/v1/scrape',\\n        {\\n          url,\\n          formats: ['markdown', 'html'],\\n          onlyMainContent: true\\n        },\\n        {\\n          headers: {\\n            'Authorization': `Bearer ${process.env.FIRECRAWL_API_KEY}`,\\n            'Content-Type': 'application/json'\\n          },\\n          timeout: 60000\\n        }\\n      );\\n      \\n      return {\\n        url,\\n        success: true,\\n        runner: 'firecrawl',\\n        data: {\\n          title: response.data.title || '',\\n          text_content: response.data.markdown || response.data.content || '',\\n          meta: {\\n            text_length: (response.data.markdown || '').length\\n          }\\n        },\\n        timestamp: new Date().toISOString(),\\n        attempts: attempt + 1\\n      };\\n      \\n    } catch (error) {\\n      if (attempt === retries - 1) {\\n        return {\\n          url,\\n          success: false,\\n          error: error.message,\\n          runner: 'firecrawl',\\n          attempts: attempt + 1\\n        };\\n      }\\n      \\n      const delay = 1000 * Math.pow(2, attempt);\\n      await new Promise(resolve => setTimeout(resolve, delay));\\n    }\\n  }\\n}\\n\\nfor (const item of failedItems) {\\n  const result = await retryFirecrawl(item.json.url);\\n  results.push(result);\\n}\\n\\nreturn results.map(r => ({ json: r }));"},"name": "Firecrawl Fallback (with Retry)","type": "n8n-nodes-base.code","position": [2450, 450]},{"parameters": {"functionCode": "// MERGE ALL RESULTS & CALCULATE STATS\\nconst allItems = $input.all();\\nconst successful = allItems.filter(item => item.json.success);\\nconst failed = allItems.filter(item => !item.json.success);\\n\\n// Group by runner\\nconst byRunner = {};\\nfor (const item of successful) {\\n  const runner = item.json.runner || 'unknown';\\n  if (!byRunner[runner]) byRunner[runner] = [];\\n  byRunner[runner].push(item.json);\\n}\\n\\n// Calculate processing times\\nconst processingTimes = successful.map(item => item.json.processingTime || 0);\\nconst avgProcessingTime = processingTimes.reduce((a, b) => a + b, 0) / processingTimes.length || 0;\\n\\nreturn {\\n  json: {\\n    results: successful.map(item => item.json),\\n    errors: failed.map(item => ({\\n      url: item.json.url,\\n      error: item.json.error,\\n      runner: item.json.runner\\n    })),\\n    stats: {\\n      total: allItems.length,\\n      successful: successful.length,\\n      failed: failed.length,\\n      success_rate: ((successful.length / allItems.length) * 100).toFixed(2) + '%',\\n      by_runner: Object.keys(byRunner).map(runner => ({\\n        runner,\\n        count: byRunner[runner].length\\n      })),\\n      avg_processing_time_ms: Math.round(avgProcessingTime)\\n    },\\n    timestamp: new Date().toISOString()\\n  }\\n};"},"name": "Merge & Calculate Stats","type": "n8n-nodes-base.code","position": [2650, 300]},{"parameters": {"operation": "executeQuery","query": "INSERT INTO scraped_data (url, title, content, metadata, runner, created_at) VALUES ($1, $2, $3, $4, $5, NOW()) ON CONFLICT (url) DO UPDATE SET title = $2, content = $3, metadata = $4, runner = $5, updated_at = NOW()","additionalFields": {"mode": "independently"}},"name": "Save to PostgreSQL (Upsert)","type": "n8n-nodes-base.postgres","position": [2850, 300],"credentials": {"postgres": {"id": "4","name": "PostgreSQL"}}},{"parameters": {"functionCode": "// STRUCTURED LOGGING FOR MONITORING\\nconst result = $json;\\n\\nconst logEntry = {\\n  timestamp: new Date().toISOString(),\\n  workflow_id: this.getWorkflow().id,\\n  execution_id: this.getExecutionId(),\\n  stats: result.stats,\\n  errors: result.errors,\\n  level: result.errors.length > 0 ? 'warning' : 'info'\\n};\\n\\nconsole.log(JSON.stringify(logEntry));\\n\\nreturn { json: result };"},"name": "Structured Logging","type": "n8n-nodes-base.code","position": [3050, 300]},{"parameters": {"respondWith": "json","responseBody": "={{JSON.stringify($json, null, 2)}}"},"name": "Return Enhanced Response","type": "n8n-nodes-base.respondToWebhook","position": [3250, 300]}],"connections": {"Webhook (Header Auth)": {"main": [[{"node": "Input Validator (Enhanced)", "type": "main", "index": 0}]]},"Input Validator (Enhanced)": {"main": [[{"node": "Split Into Batches", "type": "main", "index": 0}]]},"Split Into Batches": {"main": [[{"node": "Protected Site?", "type": "main", "index": 0}]]},"Protected Site?": {"main": [[[{"node": "Trigger Nodriver Batch (GitHub Actions)", "type": "main", "index": 0}],[{"node": "Needs JavaScript?", "type": "main", "index": 0}]]},"Needs JavaScript?": {"main": [[[{"node": "Trigger Playwright Batch (GitHub Actions)", "type": "main", "index": 0}],[{"node": "Basic HTTP Batch", "type": "main", "index": 0}]]},"Trigger Nodriver Batch (GitHub Actions)": {"main": [[{"node": "Poll GitHub Actions Status", "type": "main", "index": 0}]]},"Trigger Playwright Batch (GitHub Actions)": {"main": [[{"node": "Poll GitHub Actions Status", "type": "main", "index": 0}]]},"Poll GitHub Actions Status": {"main": [[{"node": "Download Artifact", "type": "main", "index": 0}]]},"Download Artifact": {"main": [[{"node": "Parse Results", "type": "main", "index": 0}]]},"Parse Results": {"main": [[{"node": "Extract & Clean Content", "type": "main", "index": 0}]]},"Basic HTTP Batch": {"main": [[{"node": "Extract & Clean Content", "type": "main", "index": 0}]]},"Extract & Clean Content": {"main": [[{"node": "Quality Check", "type": "main", "index": 0}]]},"Quality Check": {"main": [[[{"node": "Merge & Calculate Stats", "type": "main", "index": 0}],[{"node": "Firecrawl Fallback (with Retry)", "type": "main", "index": 0}]]},"Firecrawl Fallback (with Retry)": {"main": [[{"node": "Merge & Calculate Stats", "type": "main", "index": 0}]]},"Merge & Calculate Stats": {"main": [[{"node": "Save to PostgreSQL (Upsert)", "type": "main", "index": 0}]]},"Save to PostgreSQL (Upsert)": {"main": [[{"node": "Structured Logging", "type": "main", "index": 0}]]},"Structured Logging": {"main": [[{"node": "Return Enhanced Response", "type": "main", "index": 0}]]}},"settings": {"executionTimeout": 600,"saveManualExecutions": true,"saveExecutionProgress": true,"executionOrder": "v1"},"tags": [{"name": "production"},{"name": "scraping"},{"name": "enhanced"}]}